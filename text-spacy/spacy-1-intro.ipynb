{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab : Spacy-1: Exploring SpaCy\n",
    "\n",
    "## Overview\n",
    "Understand basic text processing and get to know SpaCy library\n",
    "\n",
    "## Run time \n",
    "20 min\n",
    "\n",
    "## SpaCy\n",
    "We will be using the excellent python text library called SpaCy.\n",
    "Here are some references\n",
    "* [SpaCy Home](http://www.spacy.io/)\n",
    "* [SpaCy book](https://course.spacy.io/)\n",
    "\n",
    "The following exercises will get you familiar with SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: SpaCy\n",
    "Let's make sure SpaCy is installed.\n",
    "You can test if SpaCy is installed by doing the following.\n",
    "\n",
    "If not you can install SpaCy as follows\n",
    "```bash\n",
    "   $   pip install spaCy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy version :  2.0.16\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# is this working?\n",
    "print (\"spacy version : \", spacy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:  Downloading SpaCy Data\n",
    "SpaCy library comes with a set of models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /home/ubuntu/apps/anaconda/lib/python3.7/site-packages (2.0.0)\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /home/ubuntu/apps/anaconda/lib/python3.7/site-packages/en_core_web_sm\n",
      "    -->\n",
      "    /home/ubuntu/apps/anaconda/lib/python3.7/site-packages/spacy/data/en_core_web_sm\n",
      "\n",
      "    You can now load the model via spacy.load('en_core_web_sm')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install the core models for English\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Exploring SpaCy datasets / corpus\n",
    "SpaCy ships with lots of data sets.  Here are a few\n",
    "* words\n",
    "* State of the Union\n",
    "* Novel (moby dick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.lang.en.English object at 0x7f9a466c31d0>\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "print (nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Words\n",
    "English words corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78942\n",
      "['\"\"', '#', '$', \"''\", ',', '-LRB-', '-RRB-', '.', ':', 'ADD', 'AFX', 'BES', 'CC', 'CD', 'DT', 'EX', 'FW', 'GW', 'HVS', 'HYPH']\n",
      "['expend', 'finisher', 'snarl', 'bewitch', 'examinee', 'haze', 'bungle', '53-floor', 'Havin', 'havin', \"Havin'\", \"havin'\", 'Havin’', 'havin’', 'Lovin', 'lovin', \"Lovin'\", \"lovin'\", 'Lovin’', 'lovin’']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.max_length = 1500000 \n",
    "\n",
    "\n",
    "words = list(nlp.vocab.strings)\n",
    "\n",
    "## TODO-2 : how many words are there?  \n",
    "## Hint : print(len(???))\n",
    "print(len(words))\n",
    "\n",
    "## TODO-2 : Print some words out\n",
    "## Hint : use w[:n] (n is a number)\n",
    "print (words[:20])\n",
    "print (words[-20:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 State of The Union text\n",
    "Let's load some data into spaCy.  We will start with the state of the union addresses.  We will look at the 2006 State of the Union from President George W. Bush."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '/data/text/state-of-the-unions/2006-GWBush.txt'\n",
    "\n",
    "## if data not found, uncomment the following\n",
    "#!wget https://elephantscale-public.s3.amazonaws.com/data/text/state-of-the-unions/2006-GWBush.txt\n",
    "#data = '2006-GWBush.txt'   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- gw2006 --\n",
      "PRESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION\n",
      "---- first 50 words ---\n",
      "[PRESIDENT, GEORGE, W., BUSH, 'S, ADDRESS, BEFORE, A, JOINT, SESSION, OF, THE, CONGRESS, ON, THE, STATE, OF, THE, UNION, \n",
      " \n",
      ", January, 31, ,, 2006, \n",
      "\n",
      ", THE, PRESIDENT, :, Thank, you, all, ., Mr., Speaker, ,, Vice, President, Cheney, ,, members, of, Congress, ,, members, of, the, Supreme, Court, and, diplomatic]\n",
      "---- first 10 sentences ---\n",
      "PRESIDENT GEORGE W. BUSH'S ADDRESS\n",
      "BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION\n",
      " \n",
      "\n",
      "January 31, 2006\n",
      "\n",
      "\n",
      "THE PRESIDENT: Thank you all.\n",
      "Mr. Speaker, Vice President Cheney, members of Congress, members of the Supreme Court and diplomatic corps, distinguished guests, and fellow citizens:\n",
      "Today our nation lost a beloved, graceful, courageous woman who called America to its founding ideals and carried on a noble dream.\n",
      "Tonight we are comforted by the hope of a glad reunion with the husband who was taken so long ago, and we are grateful for the good life of Coretta Scott King.\n",
      "(Applause.)\n",
      "\n",
      "\n",
      "President George W. Bush reacts to applause during his State of the Union Address at the Capitol, Tuesday, Jan. 31, 2006.\n",
      "White House photo by Eric DraperEvery time I'm invited to this rostrum\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "## TODO-3 : Extract the speech for 2006\n",
    "gw2006 = nlp(open(data).read())\n",
    "print(\"-- gw2006 --\")\n",
    "print (gw2006[:10])\n",
    "\n",
    "\n",
    "## TODO-4 : Extract words for the same speech\n",
    "# HINT: replace ??? with gw2006\n",
    "print (\"---- first 50 words ---\")\n",
    "gw2006_words = [token for token in gw2006]\n",
    "print(gw2006_words[:50])\n",
    "\n",
    "## TODO-6 : Extract sentences\n",
    "print ('---- first 10 sentences ---')\n",
    "gw2006_sents = [sentence for sentence in gw2006.sents]\n",
    "# print (gw2006_sents[:10])\n",
    "# ## Print first 10 sentences; HINT use [:10]\n",
    "for sentence in gw2006_sents[:10]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at the token values for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token.text :  PRESIDENT\n",
      "    token.lemma_ :  president\n",
      "    token.pos_ :  PROPN\n",
      "    token.tag_ :  NNP\n",
      "    token.dep_ :  compound\n",
      "    token.shape_ :  XXXX\n",
      "    token.is_alpha :  True\n",
      "    token.is_stop :  False\n",
      "\n",
      "token.text :  GEORGE\n",
      "    token.lemma_ :  george\n",
      "    token.pos_ :  PROPN\n",
      "    token.tag_ :  NNP\n",
      "    token.dep_ :  compound\n",
      "    token.shape_ :  XXXX\n",
      "    token.is_alpha :  True\n",
      "    token.is_stop :  False\n",
      "\n",
      "token.text :  W.\n",
      "    token.lemma_ :  w.\n",
      "    token.pos_ :  PROPN\n",
      "    token.tag_ :  NNP\n",
      "    token.dep_ :  compound\n",
      "    token.shape_ :  X.\n",
      "    token.is_alpha :  False\n",
      "    token.is_stop :  False\n",
      "\n",
      "token.text :  BUSH\n",
      "    token.lemma_ :  bush\n",
      "    token.pos_ :  PROPN\n",
      "    token.tag_ :  NNP\n",
      "    token.dep_ :  nsubj\n",
      "    token.shape_ :  XXXX\n",
      "    token.is_alpha :  True\n",
      "    token.is_stop :  False\n",
      "\n",
      "token.text :  'S\n",
      "    token.lemma_ :  's\n",
      "    token.pos_ :  PART\n",
      "    token.tag_ :  POS\n",
      "    token.dep_ :  ROOT\n",
      "    token.shape_ :  'X\n",
      "    token.is_alpha :  False\n",
      "    token.is_stop :  False\n",
      "\n",
      "token.text :  ADDRESS\n",
      "    token.lemma_ :  address\n",
      "    token.pos_ :  PROPN\n",
      "    token.tag_ :  NNP\n",
      "    token.dep_ :  attr\n",
      "    token.shape_ :  XXXX\n",
      "    token.is_alpha :  True\n",
      "    token.is_stop :  False\n",
      "\n",
      "token.text :  BEFORE\n",
      "    token.lemma_ :  before\n",
      "    token.pos_ :  ADP\n",
      "    token.tag_ :  IN\n",
      "    token.dep_ :  ROOT\n",
      "    token.shape_ :  XXXX\n",
      "    token.is_alpha :  True\n",
      "    token.is_stop :  False\n",
      "\n",
      "token.text :  A\n",
      "    token.lemma_ :  a\n",
      "    token.pos_ :  DET\n",
      "    token.tag_ :  DT\n",
      "    token.dep_ :  det\n",
      "    token.shape_ :  X\n",
      "    token.is_alpha :  True\n",
      "    token.is_stop :  False\n",
      "\n",
      "token.text :  JOINT\n",
      "    token.lemma_ :  joint\n",
      "    token.pos_ :  PROPN\n",
      "    token.tag_ :  NNP\n",
      "    token.dep_ :  compound\n",
      "    token.shape_ :  XXXX\n",
      "    token.is_alpha :  True\n",
      "    token.is_stop :  False\n",
      "\n",
      "token.text :  SESSION\n",
      "    token.lemma_ :  session\n",
      "    token.pos_ :  NOUN\n",
      "    token.tag_ :  NN\n",
      "    token.dep_ :  pobj\n",
      "    token.shape_ :  XXXX\n",
      "    token.is_alpha :  True\n",
      "    token.is_stop :  False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's see the token values for each\n",
    "\n",
    "# Run this:\n",
    "for token in gw2006[:10]:\n",
    "    print (\"token.text : \", token.text)\n",
    "    print (\"    token.lemma_ : \", token.lemma_)\n",
    "    print (\"    token.pos_ : \", token.pos_)\n",
    "    print (\"    token.tag_ : \", token.tag_)\n",
    "    print (\"    token.dep_ : \", token.dep_)\n",
    "    print (\"    token.shape_ : \", token.shape_)\n",
    "    print (\"    token.is_alpha : \", token.is_alpha)\n",
    "    print (\"    token.is_stop : \", token.is_stop)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Novels!\n",
    "Let's look at some classic novels, such as [Moby Dick](https://elephantscale-public.s3.amazonaws.com/data/text/books/moby-dick.txt)\n",
    "\n",
    "We need more memory for this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '/data/text/books/moby-dick.txt'\n",
    "\n",
    "## if data not found, uncomment the following\n",
    "# !wget https://elephantscale-public.s3.amazonaws.com/data/text/books/moby-dick.txt\n",
    "#data = 'moby-dick.txt'   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 23s, sys: 25.5 s, total: 1min 49s\n",
      "Wall time: 27.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "moby_dick = nlp(open(data).read())\n",
    "\n",
    "# print(moby_dick)\n",
    "# print (moby_dick[1:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(words) :  281789\n",
      "---- first 100 words ---\n",
      "[﻿The, Project, Gutenberg, EBook, of, Moby, Dick, ;, or, The, Whale, ,, by, Herman, Melville, \n",
      "\n",
      ", This, eBook, is, for, the, use, of, anyone, anywhere, at, no, cost, and, with, \n",
      ", almost, no, restrictions, whatsoever, .,  , You, may, copy, it, ,, give, it, away, or, \n",
      ", re, -, use, it, under, the, terms, of, the, Project, Gutenberg, License, included, \n",
      ", with, this, eBook, or, online, at, www.gutenberg.org, \n",
      "\n",
      "\n",
      ", Title, :, Moby, Dick, ;, or, The, Whale, \n",
      "\n",
      ", Author, :, Herman, Melville, \n",
      "\n",
      ", Last, Updated, :, January, 3, ,, 2009, \n",
      ", Posting, Date, :, December, 25, ,, 2008, [, EBook]\n"
     ]
    }
   ],
   "source": [
    "# TODO-2 : Get words for Moby Dick\n",
    "# moby_dick_words = \n",
    "# TODO-3 print the number of words (Hint : len)\n",
    "# TODO-4 : print first 100 words\n",
    "\n",
    "moby_dick_words = [token for token in moby_dick]\n",
    "\n",
    "print(\"len(words) : \", len(moby_dick_words))\n",
    "print (\"---- first 100 words ---\")\n",
    "print(moby_dick_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Tokenizing Your Own Text\n",
    "We are going to use spacy library to tokenize texts.  There are 3 tokenizers we are goign to test\n",
    "\n",
    "Try the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:\n",
      " I went to Starbucks. I bought a latte for $4.50!\n",
      "Yum :-)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "I went to Starbucks. I bought a latte for $4.50!\n",
       "Yum :-)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TODO : feel free to change this to your own text\n",
    "text=\"\"\"I went to Starbucks. I bought a latte for $4.50!\n",
    "Yum :-)\n",
    "\"\"\"\n",
    "\n",
    "print(\"text:\\n\", text)\n",
    "print(\"\")\n",
    "\n",
    "starbucks = nlp(text)\n",
    "starbucks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Try sentence tokenization\n",
    "HINT: use `.sents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I went to Starbucks., I bought a latte for $4.50!\n",
      ", Yum :-)\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(list(starbucks.sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Try tokenizing on words and punctuation\n",
    "\n",
    "HINT: Just convert starbucks into a list, or do a for loop on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I, went, to, Starbucks, ., I, bought, a, latte, for, $, 4.50, !, \n",
      ", Yum, :-), \n",
      "]\n",
      "[I, went, to, Starbucks, ., I, bought, a, latte, for, $, 4.50, !, \n",
      ", Yum, :-), \n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(list(starbucks))\n",
    "print([word for word in starbucks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Try getting **ONLY** words, not punctuation\n",
    "\n",
    "What we'll do is try to to get ONLY words, no punctuation.  Let's look at see how SpaCy tokenizes this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token.text :  I\n",
      "    token.lemma_ :  -PRON-\n",
      "    token.pos_ :  PRON\n",
      "    token.tag_ :  PRP\n",
      "    token.dep_ :  nsubj\n",
      "    token.shape_ :  X\n",
      "    token.is_alpha :  True\n",
      "    token.is_stop :  False\n",
      "\n",
      "token.text :  went\n",
      "    token.lemma_ :  go\n",
      "    token.pos_ :  VERB\n",
      "    token.tag_ :  VBD\n",
      "    token.dep_ :  ROOT\n",
      "    token.shape_ :  xxxx\n",
      "    token.is_alpha :  True\n",
      "    token.is_stop :  False\n",
      "\n",
      "token.text :  to\n",
      "    token.lemma_ :  to\n",
      "    token.pos_ :  ADP\n",
      "    token.tag_ :  IN\n",
      "    token.dep_ :  prep\n",
      "    token.shape_ :  xx\n",
      "    token.is_alpha :  True\n",
      "    token.is_stop :  True\n",
      "\n",
      "token.text :  Starbucks\n",
      "    token.lemma_ :  starbucks\n",
      "    token.pos_ :  PROPN\n",
      "    token.tag_ :  NNP\n",
      "    token.dep_ :  pobj\n",
      "    token.shape_ :  Xxxxx\n",
      "    token.is_alpha :  True\n",
      "    token.is_stop :  False\n",
      "\n",
      "token.text :  .\n",
      "    token.lemma_ :  .\n",
      "    token.pos_ :  PUNCT\n",
      "    token.tag_ :  .\n",
      "    token.dep_ :  punct\n",
      "    token.shape_ :  .\n",
      "    token.is_alpha :  False\n",
      "    token.is_stop :  False\n",
      "\n",
      "token.text :  I\n",
      "    token.lemma_ :  -PRON-\n",
      "    token.pos_ :  PRON\n",
      "    token.tag_ :  PRP\n",
      "    token.dep_ :  nsubj\n",
      "    token.shape_ :  X\n",
      "    token.is_alpha :  True\n",
      "    token.is_stop :  False\n",
      "\n",
      "token.text :  bought\n",
      "    token.lemma_ :  buy\n",
      "    token.pos_ :  VERB\n",
      "    token.tag_ :  VBD\n",
      "    token.dep_ :  ROOT\n",
      "    token.shape_ :  xxxx\n",
      "    token.is_alpha :  True\n",
      "    token.is_stop :  False\n",
      "\n",
      "token.text :  a\n",
      "    token.lemma_ :  a\n",
      "    token.pos_ :  DET\n",
      "    token.tag_ :  DT\n",
      "    token.dep_ :  det\n",
      "    token.shape_ :  x\n",
      "    token.is_alpha :  True\n",
      "    token.is_stop :  True\n",
      "\n",
      "token.text :  latte\n",
      "    token.lemma_ :  latte\n",
      "    token.pos_ :  NOUN\n",
      "    token.tag_ :  NN\n",
      "    token.dep_ :  dobj\n",
      "    token.shape_ :  xxxx\n",
      "    token.is_alpha :  True\n",
      "    token.is_stop :  False\n",
      "\n",
      "token.text :  for\n",
      "    token.lemma_ :  for\n",
      "    token.pos_ :  ADP\n",
      "    token.tag_ :  IN\n",
      "    token.dep_ :  prep\n",
      "    token.shape_ :  xxx\n",
      "    token.is_alpha :  True\n",
      "    token.is_stop :  True\n",
      "\n",
      "token.text :  $\n",
      "    token.lemma_ :  $\n",
      "    token.pos_ :  SYM\n",
      "    token.tag_ :  $\n",
      "    token.dep_ :  nmod\n",
      "    token.shape_ :  $\n",
      "    token.is_alpha :  False\n",
      "    token.is_stop :  False\n",
      "\n",
      "token.text :  4.50\n",
      "    token.lemma_ :  4.50\n",
      "    token.pos_ :  NUM\n",
      "    token.tag_ :  CD\n",
      "    token.dep_ :  pobj\n",
      "    token.shape_ :  d.dd\n",
      "    token.is_alpha :  False\n",
      "    token.is_stop :  False\n",
      "\n",
      "token.text :  !\n",
      "    token.lemma_ :  !\n",
      "    token.pos_ :  PUNCT\n",
      "    token.tag_ :  .\n",
      "    token.dep_ :  punct\n",
      "    token.shape_ :  !\n",
      "    token.is_alpha :  False\n",
      "    token.is_stop :  False\n",
      "\n",
      "token.text :  \n",
      "\n",
      "    token.lemma_ :  \n",
      "\n",
      "    token.pos_ :  SPACE\n",
      "    token.tag_ :  \n",
      "    token.dep_ :  \n",
      "    token.shape_ :  \n",
      "\n",
      "    token.is_alpha :  False\n",
      "    token.is_stop :  False\n",
      "\n",
      "token.text :  Yum\n",
      "    token.lemma_ :  yum\n",
      "    token.pos_ :  PROPN\n",
      "    token.tag_ :  NNP\n",
      "    token.dep_ :  ROOT\n",
      "    token.shape_ :  Xxx\n",
      "    token.is_alpha :  True\n",
      "    token.is_stop :  False\n",
      "\n",
      "token.text :  :-)\n",
      "    token.lemma_ :  :-)\n",
      "    token.pos_ :  PUNCT\n",
      "    token.tag_ :  NFP\n",
      "    token.dep_ :  punct\n",
      "    token.shape_ :  :-)\n",
      "    token.is_alpha :  False\n",
      "    token.is_stop :  False\n",
      "\n",
      "token.text :  \n",
      "\n",
      "    token.lemma_ :  \n",
      "\n",
      "    token.pos_ :  SPACE\n",
      "    token.tag_ :  \n",
      "    token.dep_ :  \n",
      "    token.shape_ :  \n",
      "\n",
      "    token.is_alpha :  False\n",
      "    token.is_stop :  False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look at the output here:\n",
    "\n",
    "for token in starbucks[:20]:\n",
    "    print (\"token.text : \", token.text)\n",
    "    print (\"    token.lemma_ : \", token.lemma_)\n",
    "    print (\"    token.pos_ : \", token.pos_)\n",
    "    print (\"    token.tag_ : \", token.tag_)\n",
    "    print (\"    token.dep_ : \", token.dep_)\n",
    "    print (\"    token.shape_ : \", token.shape_)\n",
    "    print (\"    token.is_alpha : \", token.is_alpha)\n",
    "    print (\"    token.is_stop : \", token.is_stop)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice anything that all the puctuation has in common? HINT: look at the `.pos_` member of the token.\n",
    "\n",
    "Can you write an expression will get all words that are not punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I, went, to, Starbucks, I, bought, a, latte, for, $, 4.50, \n",
      ", Yum, \n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print([word for word in starbucks if word.pos_ != 'PUNCT']) # TODO: Fix this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
