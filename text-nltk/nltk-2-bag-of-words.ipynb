{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Text With NLTK\n",
    "\n",
    "## Overview\n",
    "Basic text analytics with NLTK\n",
    "\n",
    "## Runtime\n",
    "20 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import nltk\n",
    "nltk.download('all')\n",
    "\n",
    "## downloading using command line\n",
    "#! python -m nltk.downloader all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Wordcount on NLTK Corpus\n",
    "Let's do some basic word counts in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1945-Truman.txt', '1946-Truman.txt', '1947-Truman.txt', '1948-Truman.txt', '1949-Truman.txt', '1950-Truman.txt', '1951-Truman.txt', '1953-Eisenhower.txt', '1954-Eisenhower.txt', '1955-Eisenhower.txt', '1956-Eisenhower.txt', '1957-Eisenhower.txt', '1958-Eisenhower.txt', '1959-Eisenhower.txt', '1960-Eisenhower.txt', '1961-Kennedy.txt', '1962-Kennedy.txt', '1963-Johnson.txt', '1963-Kennedy.txt', '1964-Johnson.txt', '1965-Johnson-1.txt', '1965-Johnson-2.txt', '1966-Johnson.txt', '1967-Johnson.txt', '1968-Johnson.txt', '1969-Johnson.txt', '1970-Nixon.txt', '1971-Nixon.txt', '1972-Nixon.txt', '1973-Nixon.txt', '1974-Nixon.txt', '1975-Ford.txt', '1976-Ford.txt', '1977-Ford.txt', '1978-Carter.txt', '1979-Carter.txt', '1980-Carter.txt', '1981-Reagan.txt', '1982-Reagan.txt', '1983-Reagan.txt', '1984-Reagan.txt', '1985-Reagan.txt', '1986-Reagan.txt', '1987-Reagan.txt', '1988-Reagan.txt', '1989-Bush.txt', '1990-Bush.txt', '1991-Bush-1.txt', '1991-Bush-2.txt', '1992-Bush.txt', '1993-Clinton.txt', '1994-Clinton.txt', '1995-Clinton.txt', '1996-Clinton.txt', '1997-Clinton.txt', '1998-Clinton.txt', '1999-Clinton.txt', '2000-Clinton.txt', '2001-GWBush-1.txt', '2001-GWBush-2.txt', '2002-GWBush.txt', '2003-GWBush.txt', '2004-GWBush.txt', '2005-GWBush.txt', '2006-GWBush.txt']\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# from os.path import expanduser\n",
    "# nltk.data.path.append( expanduser(\"~\") + \"/data/nltk_data\")\n",
    "from nltk.corpus import state_union\n",
    "from nltk import FreqDist\n",
    "\n",
    "## -- state of the union\n",
    "print(state_union.fileids())\n",
    "print(\"---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of gw2006 :  33411\n",
      "---- gw2006-----\n",
      "RESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION\n",
      " \n",
      "January 31, 2006\n",
      "\n",
      "THE PRESIDENT: Thank you all. Mr. Speaker, Vice President Cheney, members of Congress, members of the Supreme Court and diplomatic corps, distinguished guests, and fellow citizens:\n",
      "---\n",
      "len(gw2006_words) :  6515\n",
      "--- gw2006_words ----\n",
      "['GEORGE', 'W', '.', 'BUSH', \"'\", 'S', 'ADDRESS', 'BEFORE', 'A', 'JOINT', 'SESSION', 'OF', 'THE', 'CONGRESS', 'ON', 'THE', 'STATE', 'OF', 'THE', 'UNION', 'January', '31', ',', '2006', 'THE', 'PRESIDENT', ':', 'Thank', 'you', 'all', '.', 'Mr', '.', 'Speaker', ',', 'Vice', 'President', 'Cheney', ',', 'members', 'of', 'Congress', ',', 'members', 'of', 'the', 'Supreme', 'Court', 'and', 'diplomatic', 'corps', ',', 'distinguished', 'guests', ',', 'and', 'fellow', 'citizens', ':', 'Today', 'our', 'nation', 'lost', 'a', 'beloved', ',', 'graceful', ',', 'courageous', 'woman', 'who', 'called', 'America', 'to', 'its', 'founding', 'ideals', 'and', 'carried', 'on', 'a', 'noble', 'dream', '.', 'Tonight', 'we', 'are', 'comforted', 'by', 'the', 'hope', 'of', 'a', 'glad', 'reunion', 'with', 'the', 'husband', 'who']\n",
      "---\n",
      "top words : \n",
      "[(',', 323),\n",
      " ('.', 285),\n",
      " ('the', 260),\n",
      " ('and', 240),\n",
      " ('of', 186),\n",
      " ('to', 170),\n",
      " ('in', 122),\n",
      " ('a', 105),\n",
      " ('our', 96),\n",
      " ('we', 85),\n",
      " ('(', 68),\n",
      " ('that', 66),\n",
      " ('.)', 65),\n",
      " ('is', 65),\n",
      " ('Applause', 64),\n",
      " ('--', 61),\n",
      " ('will', 59),\n",
      " ('America', 43),\n",
      " ('We', 38),\n",
      " (\"'\", 37),\n",
      " ('by', 37),\n",
      " ('are', 36),\n",
      " ('have', 34),\n",
      " ('I', 32),\n",
      " ('for', 30),\n",
      " ('-', 28),\n",
      " ('world', 27),\n",
      " ('with', 26),\n",
      " ('not', 26),\n",
      " ('this', 25),\n",
      " ('has', 25),\n",
      " ('be', 23),\n",
      " ('or', 23),\n",
      " ('from', 23),\n",
      " ('more', 23),\n",
      " ('on', 22),\n",
      " ('it', 22),\n",
      " ('us', 22),\n",
      " ('people', 22),\n",
      " ('must', 20),\n",
      " ('their', 20),\n",
      " ('all', 19),\n",
      " ('And', 19),\n",
      " ('The', 18),\n",
      " ('American', 18),\n",
      " ('nation', 17),\n",
      " ('at', 17),\n",
      " ('country', 17),\n",
      " ('freedom', 17),\n",
      " ('than', 17)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "gw2006 = state_union.raw('2006-GWBush.txt')\n",
    "print (\"len of gw2006 : \" , len(gw2006))\n",
    "print (\"---- gw2006-----\")\n",
    "print(gw2006[1:300])\n",
    "print(\"---\")\n",
    "\n",
    "## TODO-2 :  get the words\n",
    "gw2006_words = state_union.words('2006-GWBush.txt')\n",
    "## TODO-3 : print number of words and some words (Hint : gw2006_words[1:100])\n",
    "print(\"len(gw2006_words) : \", len(gw2006_words))\n",
    "print (\"--- gw2006_words ----\")\n",
    "print (gw2006_words[1:100])\n",
    "print(\"---\")\n",
    "\n",
    "fdist = nltk.FreqDist(gw2006_words)\n",
    "print (\"top words : \" )\n",
    "top_words = fdist.most_common(50)\n",
    "pprint(top_words)\n",
    "## What do we see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Cleaning up text\n",
    "In the previous example, our top words were 'the', 'and' , 'of'.  These are called 'stop words'.  Let's clean them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(gw2006_words) :  6515\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# from os.path import expanduser\n",
    "# nltk.data.path.append( expanduser(\"~\") + \"/data/nltk_data\")\n",
    "\n",
    "from nltk.corpus import state_union\n",
    "\n",
    "gw2006_words = state_union.words('2006-GWBush.txt')\n",
    "gw2006_words_lower = [i.lower() for i in gw2006_words]\n",
    "print(\"len(gw2006_words) : \", len(gw2006_words))\n",
    "print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(stop_words_en) :  179\n",
      "stop words_en : \n",
      " ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "## Stop words \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "print (\"len(stop_words_en) : \", len(stop_words_en))\n",
    "print(\"stop words_en : \\n\", sorted(stop_words_en))\n",
    "print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(cleaned) :  3975\n",
      "---\n",
      "cleaned text:\n",
      " ['george', 'w', '.', 'bush', \"'\", 'address', 'joint', 'session', 'congress', 'state', 'union', 'january', '31', ',', '2006', 'president', ':', 'thank', '.', 'mr', '.', 'speaker', ',', 'vice', 'president', 'cheney', ',', 'members', 'congress', ',', 'members', 'supreme', 'court', 'diplomatic', 'corps', ',', 'distinguished', 'guests', ',', 'fellow', 'citizens', ':', 'today', 'nation', 'lost', 'beloved', ',', 'graceful', ',', 'courageous', 'woman', 'called', 'america', 'founding', 'ideals', 'carried', 'noble', 'dream', '.', 'tonight', 'comforted', 'hope', 'glad', 'reunion', 'husband', 'taken', 'long', 'ago', ',', 'grateful', 'good', 'life', 'coretta', 'scott', 'king', '.', '(', 'applause', '.)', 'president', 'george', 'w', '.', 'bush', 'reacts', 'applause', 'state', 'union', 'address', 'capitol', ',', 'tuesday', ',', 'jan', '.', '31', ',', '2006', '.', 'white', 'house', 'photo', 'eric', 'draperevery', 'time', \"'\", 'invited', 'rostrum', ',', \"'\", 'humbled', 'privilege', ',', 'mindful', 'history', \"'\", 'seen', 'together', '.', 'gathered', 'capitol', 'dome', 'moments', 'national', 'mourning', 'national', 'achievement', '.', 'served', 'america', 'one', 'consequential', 'periods', 'history', '--', 'honor', 'serve', '.', 'system', 'two', 'parties', ',', 'two', 'chambers', ',', 'two', 'elected', 'branches', ',', 'always', 'differences', 'debate', '.', 'even', 'tough', 'debates', 'conducted', 'civil', 'tone', ',', 'differences', 'cannot', 'allowed', 'harden', 'anger', '.', 'confront', 'great', 'issues', 'us', ',', 'must', 'act', 'spirit', 'goodwill', 'respect', 'one', 'another', '--', 'part', '.', 'tonight', 'state', 'union', 'strong', '--', 'together', 'make', 'stronger', '.', '(', 'applause', '.)', 'decisive', 'year', ',', 'make', 'choices', 'determine']\n",
      "---\n",
      "most common words in cleaned :\n",
      " [(',', 323), ('.', 285), ('(', 68), ('applause', 67), ('.)', 65), ('--', 61), ('america', 43), (\"'\", 37), ('-', 28), ('world', 27), ('us', 22), ('people', 22), ('must', 20), ('american', 18), ('nation', 17), ('country', 17), ('freedom', 17), ('economy', 16), (':', 15), ('congress', 14), ('new', 14), ('also', 14), ('members', 11), ('tonight', 11), ('one', 11), ('make', 11), ('year', 11), ('every', 11), ('government', 11), ('years', 11), ('americans', 11), ('life', 10), ('act', 10), ('yet', 10), ('states', 10), ('lead', 10), ('security', 10), ('support', 10), ('president', 9), ('citizens', 9), ('two', 9), ('great', 9), ('united', 9), ('fight', 9), ('would', 9), ('hopeful', 9), ('state', 8), ('union', 8), ('hope', 8), ('house', 8)]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "cleaned = [i for i in gw2006_words_lower if i not in stop_words_en]\n",
    "## TODO : how many words in cleaned\n",
    "print(\"len(cleaned) : \", len(cleaned))\n",
    "print(\"---\")\n",
    "\n",
    "print(\"cleaned text:\\n\", cleaned[1:200])\n",
    "print(\"---\")\n",
    "\n",
    "fdist = nltk.FreqDist(cleaned)\n",
    "print (\"most common words in cleaned :\\n\" , fdist.most_common(50))\n",
    "print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(cleaned2) :  3210\n",
      "---\n",
      "most common words in cleaned2\n",
      "[('applause', 67),\n",
      " ('.)', 65),\n",
      " ('--', 61),\n",
      " ('america', 43),\n",
      " ('world', 27),\n",
      " ('us', 22),\n",
      " ('people', 22),\n",
      " ('must', 20),\n",
      " ('american', 18),\n",
      " ('nation', 17),\n",
      " ('country', 17),\n",
      " ('freedom', 17),\n",
      " ('economy', 16),\n",
      " ('congress', 14),\n",
      " ('new', 14),\n",
      " ('also', 14),\n",
      " ('members', 11),\n",
      " ('tonight', 11),\n",
      " ('one', 11),\n",
      " ('make', 11),\n",
      " ('year', 11),\n",
      " ('every', 11),\n",
      " ('government', 11),\n",
      " ('years', 11),\n",
      " ('americans', 11),\n",
      " ('life', 10),\n",
      " ('act', 10),\n",
      " ('yet', 10),\n",
      " ('states', 10),\n",
      " ('lead', 10),\n",
      " ('security', 10),\n",
      " ('support', 10),\n",
      " ('president', 9),\n",
      " ('citizens', 9),\n",
      " ('two', 9),\n",
      " ('great', 9),\n",
      " ('united', 9),\n",
      " ('fight', 9),\n",
      " ('would', 9),\n",
      " ('hopeful', 9),\n",
      " ('state', 8),\n",
      " ('union', 8),\n",
      " ('hope', 8),\n",
      " ('house', 8),\n",
      " ('like', 8),\n",
      " ('military', 8),\n",
      " ('never', 8),\n",
      " ('know', 8),\n",
      " ('need', 8),\n",
      " ('energy', 8)]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# remove punctuation\n",
    "stop_words_en.update(['-', '.', ',', '#', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}']) \n",
    "\n",
    "cleaned2 = [i for i in gw2006_words_lower if i not in stop_words_en]\n",
    "## TODO : how many words in cleaned\n",
    "print(\"len(cleaned2) : \", len(cleaned2))\n",
    "print(\"---\")\n",
    "\n",
    "## TODO - calculate FreqDist for cleaned2\n",
    "fdist = nltk.FreqDist(cleaned2)\n",
    "print (\"most common words in cleaned2\")\n",
    "pprint(fdist.most_common(50))\n",
    "print(\"---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(cleaned3) :  3084\n",
      "---\n",
      "most common words in cleaned3\n",
      "[('applause', 67),\n",
      " ('america', 43),\n",
      " ('world', 27),\n",
      " ('us', 22),\n",
      " ('people', 22),\n",
      " ('must', 20),\n",
      " ('american', 18),\n",
      " ('nation', 17),\n",
      " ('country', 17),\n",
      " ('freedom', 17),\n",
      " ('economy', 16),\n",
      " ('congress', 14),\n",
      " ('new', 14),\n",
      " ('also', 14),\n",
      " ('members', 11),\n",
      " ('tonight', 11),\n",
      " ('one', 11),\n",
      " ('make', 11),\n",
      " ('year', 11),\n",
      " ('every', 11),\n",
      " ('government', 11),\n",
      " ('years', 11),\n",
      " ('americans', 11),\n",
      " ('life', 10),\n",
      " ('act', 10),\n",
      " ('yet', 10),\n",
      " ('states', 10),\n",
      " ('lead', 10),\n",
      " ('security', 10),\n",
      " ('support', 10),\n",
      " ('president', 9),\n",
      " ('citizens', 9),\n",
      " ('two', 9),\n",
      " ('great', 9),\n",
      " ('united', 9),\n",
      " ('fight', 9),\n",
      " ('would', 9),\n",
      " ('hopeful', 9),\n",
      " ('state', 8),\n",
      " ('union', 8),\n",
      " ('hope', 8),\n",
      " ('house', 8),\n",
      " ('like', 8),\n",
      " ('military', 8),\n",
      " ('never', 8),\n",
      " ('know', 8),\n",
      " ('need', 8),\n",
      " ('energy', 8),\n",
      " ('competitive', 8),\n",
      " ('tax', 8)]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "## TODO :  further cleanup\n",
    "## Inspecting the output, we see a couple of punctuations that need cleaning up\n",
    "## add them to stop words list and clean up again\n",
    "stop_words_en.update(['.)', '??', '--']) \n",
    "cleaned3 = [i for i in gw2006_words_lower if i not in stop_words_en]\n",
    "## TODO : len of cleaned3\n",
    "print(\"len(cleaned3) : \", len(cleaned3))\n",
    "print(\"---\")\n",
    "\n",
    "## TODO : Commpute FreqDist for cleaned3\n",
    "fdist = nltk.FreqDist(cleaned3)\n",
    "print (\"most common words in cleaned3\" )\n",
    "pprint(fdist.most_common(50))\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Analyze another text dataset\n",
    "In the previous exercise we analyzed data bundled with NLTK.\n",
    "In this section, we are going to load and analyze custom dataset\n",
    "\n",
    "We will use [State of the union 2014 by President Obama](../data/text/sotu-2014-obama.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(text) 42605\n",
      "text:\n",
      " RESIDENT OBAMA: Thank you. (Applause.) Thank you so much. Thank you. Thank you. (Applause.) Thank you so much.\n",
      "\n",
      "Mr. Speaker, Mr. Vice President, members of Congress, my fellow Americans, today in America, a teacher spent extra time with a student who needed it and did her part to lift America's graduation rate to its highest levels in more than three decades.\n",
      "\n",
      "An entrepreneur flipped on the lights in her tech startup and did her part to add to the more than 8 million new jobs our businesses hav\n",
      "---\n",
      "len(words) :  8568\n",
      "words:\n",
      " ['PRESIDENT', 'OBAMA', ':', 'Thank', 'you', '.', '(', 'Applause', '.', ')', 'Thank', 'you', 'so', 'much', '.', 'Thank', 'you', '.', 'Thank', 'you', '.', '(', 'Applause', '.', ')', 'Thank', 'you', 'so', 'much', '.', 'Mr.', 'Speaker', ',', 'Mr.', 'Vice', 'President', ',', 'members', 'of', 'Congress', ',', 'my', 'fellow', 'Americans', ',', 'today', 'in', 'America', ',', 'a']\n",
      "---\n",
      "words_lower:\n",
      " ['president', 'obama', ':', 'thank', 'you', '.', '(', 'applause', '.', ')', 'thank', 'you', 'so', 'much', '.', 'thank', 'you', '.', 'thank', 'you', '.', '(', 'applause', '.', ')', 'thank', 'you', 'so', 'much', '.', 'mr.', 'speaker', ',', 'mr.', 'vice', 'president', ',', 'members', 'of', 'congress', ',', 'my', 'fellow', 'americans', ',', 'today', 'in', 'america', ',', 'a']\n",
      "---\n",
      "most common in words :\n",
      " [('.', 454), (',', 422), ('the', 271), ('to', 263), ('and', 205), ('of', 151), ('a', 125), ('our', 114), ('that', 113), ('we', 108), ('(', 106), (')', 106), (\"'s\", 105), ('in', 104), ('for', 70), ('I', 69), ('more', 64), ('is', 64), ('--', 60), ('with', 52), ('on', 50), ('And', 50), ('Applause', 49), ('this', 49), ('applause', 48), ('have', 45), ('not', 42), ('will', 40), ('America', 39), ('can', 39), ('it', 35), ('their', 35), ('do', 32), ('help', 31), ('by', 31), ('you', 30), ('are', 29), ('than', 28), ('new', 28), ('But', 28), ('work', 28), ('Americans', 27), ('Cheers', 27), ('as', 26), ('jobs', 25), ('from', 25), ('all', 25), ('has', 25), ('people', 24), ('or', 24)]\n",
      "---\n",
      "len(cleaned1) :  5284\n",
      "---\n",
      "most common in cleaned1 :\n",
      " [('.', 454), (',', 422), ('(', 106), (')', 106), (\"'s\", 105), ('applause', 97), ('--', 60), ('america', 39), ('help', 32), ('cheers', 32), ('new', 29), ('work', 28), ('americans', 27), ('jobs', 25), ('people', 24), ('every', 24), ('let', 24), (\"n't\", 24), (\"'re\", 23), ('make', 22), (';', 22), ('american', 21), ('congress', 20), ('one', 20), ('us', 20), ('get', 20), ('years', 19), ('like', 19), ('today', 17), ('year', 17), ('country', 16), ('time', 15), ('businesses', 15), ('first', 15), ('know', 15), (':', 14), ('world', 14), ('give', 14), ('states', 14), (\"'ll\", 14), ('tonight', 13), ('together', 13), ('job', 13), ('need', 13), ('support', 13), ('home', 12), ('opportunity', 12), ('working', 12), ('economy', 12), (\"'ve\", 12)]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# from os.path import expanduser\n",
    "# nltk.data.path.append( expanduser(\"~\") + \"/data/nltk_data\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "f = open('/data/text/state-of-the-unions/2014-Obama.txt')\n",
    "text = f.read()\n",
    "print(\"len(text)\", len(text))\n",
    "print(\"text:\\n\", text[1:500])\n",
    "print(\"---\")\n",
    "\n",
    "# Tokenize, split into words\n",
    "words = word_tokenize(text)\n",
    "words_lower = [i.lower() for i in words]\n",
    "print(\"len(words) : \", len(words))\n",
    "print(\"words:\\n\", words[:50])\n",
    "print (\"---\")\n",
    "print(\"words_lower:\\n\", words_lower[:50])\n",
    "print (\"---\")\n",
    "\n",
    "# fdist\n",
    "fdist = nltk.FreqDist(words)\n",
    "print (\"most common in words :\\n\" , fdist.most_common(50))\n",
    "print (\"---\")\n",
    "\n",
    "\n",
    "## TODO : now use the above example to eliminate stop words from text\n",
    "## and run distribution\n",
    "\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "\n",
    "cleaned1 = [i for i in words_lower if i not in stop_words_en]\n",
    "print(\"len(cleaned1) : \", len(cleaned1))\n",
    "print(\"---\")\n",
    "\n",
    "fdist = nltk.FreqDist(cleaned1)\n",
    "print (\"most common in cleaned1 :\\n\" , fdist.most_common(50))\n",
    "print (\"---\")\n",
    "\n",
    "## TODO : continue cleaning up further\n",
    "## remove punctuation\n",
    "## remove any other \n",
    "\n",
    "## Final output is \n",
    "## [('applause', 97), ('america', 39), ('help', 32), ('cheers', 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Stemming Part 1\n",
    "Let's explore different stemming algorithm available in NLTK.  Run the code below and observe the differences in stemming (marked by \\*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word,  snowball_stem,   porter_stem, lancaster stem\n",
      "run,  run,  run,  run\n",
      "running,  run,  run,  run\n",
      "* like,  like,  like,  lik\n",
      "* liked,  like,  like,  lik\n",
      "snow,  snow,  snow,  snow\n",
      "snowing,  snow,  snow,  snow\n",
      "dog,  dog,  dog,  dog\n",
      "dogs,  dog,  dog,  dog\n",
      "* maximum,  maximum,  maximum,  maxim\n",
      "* multiply,  multipli,  multipli,  multiply\n",
      "* crying,  cri,  cri,  cry\n",
      "leaves,  leav,  leav,  leav\n",
      "* fairly,  fair,  fairli,  fair\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# from os.path import expanduser\n",
    "# nltk.data.path.append( expanduser(\"~\") + \"/data/nltk_data\")\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "words = ['run', 'running', 'like', 'liked', 'snow', 'snowing', 'dog', 'dogs', 'maximum', \n",
    "         'multiply', 'crying', 'leaves', 'fairly']\n",
    "#print(words)\n",
    "stemmer_snowball = SnowballStemmer(\"english\")\n",
    "stemmer_porter = PorterStemmer()\n",
    "stemmer_lancaster = LancasterStemmer()\n",
    "#stemmed_tokens =  [stemmer.stem(word) for word in words]\n",
    "#print(stemmed_tokens)\n",
    "\n",
    "# run through a few stems\n",
    "print(\"word,  snowball_stem,   porter_stem, lancaster stem\")\n",
    "for w in words:\n",
    "    snowball_stem = stemmer_snowball.stem(w)\n",
    "    porter_stem = stemmer_porter.stem(w)\n",
    "    lancaster_stem = stemmer_lancaster.stem(w)\n",
    "    if (snowball_stem != porter_stem) or (snowball_stem != lancaster_stem) or (porter_stem != lancaster_stem):\n",
    "        print(\"* \", end='')\n",
    "    print (\"{},  {},  {},  {}\".format(w, snowball_stem, porter_stem, lancaster_stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Putting it all together\n",
    "Now that we know all the algorithms, let's do a final analysis on 2014 SOTU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(cleaned1) :  3991\n",
      "cleaned1:\n",
      " ['president', 'obama', 'thank', 'applause', 'thank', 'much', 'thank', 'thank', 'applause', 'thank', 'much', 'mr.', 'speaker', 'mr.', 'vice', 'president', 'members', 'congress', 'fellow', 'americans', 'today', 'america', 'teacher', 'spent', 'extra', 'time', 'student', 'needed', 'part', 'lift', 'america', 'graduation', 'rate', 'highest', 'levels', 'three', 'decades', 'entrepreneur', 'flipped', 'lights', 'tech', 'startup', 'part', 'add', '8', 'million', 'new', 'jobs', 'businesses', 'created']\n",
      "---\n",
      "len(cleaned2) :  3991\n",
      "cleaned2:\n",
      " ['presid', 'obama', 'thank', 'applaus', 'thank', 'much', 'thank', 'thank', 'applaus', 'thank', 'much', 'mr.', 'speaker', 'mr.', 'vice', 'presid', 'member', 'congress', 'fellow', 'american', 'today', 'america', 'teacher', 'spent', 'extra', 'time', 'student', 'need', 'part', 'lift', 'america', 'graduat', 'rate', 'highest', 'level', 'three', 'decad', 'entrepreneur', 'flip', 'light', 'tech', 'startup', 'part', 'add', '8', 'million', 'new', 'job', 'busi', 'creat']\n",
      "---\n",
      "top 20 word count(cleaned2) : \n",
      "applaus = 97\n",
      "american = 48\n",
      "work = 44\n",
      "help = 41\n",
      "america = 40\n",
      "job = 38\n",
      "year = 36\n",
      "cheer = 33\n",
      "new = 29\n",
      "make = 25\n",
      "let = 25\n",
      "peopl = 24\n",
      "everi = 24\n",
      "n't = 24\n",
      "busi = 23\n",
      "re = 23\n",
      "need = 21\n",
      "one = 21\n",
      "congress = 20\n",
      "state = 20\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# from os.path import expanduser\n",
    "# nltk.data.path.append( expanduser(\"~\") + \"/data/nltk_data\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "f = open('/data/text/state-of-the-unions/2014-Obama.txt')\n",
    "text = f.read()\n",
    "words = word_tokenize(text)\n",
    "\n",
    "## TODO : Cleanup 1 : lowercase it all\n",
    "words_lower = [i.lower() for i in words]\n",
    "\n",
    "\n",
    "## cleanup 2 - remove stop words\n",
    "stop_words_english = set(stopwords.words('english'))\n",
    "stop_words_english.update(['-', '.', ',', '#', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "stop_words_english.update([\"'s\", '--'])\n",
    "## TODO : iterate through 'words_lower'\n",
    "cleaned1 = [i for i in words_lower if i not in stop_words_english]\n",
    "print(\"len(cleaned1) : \", len(cleaned1))\n",
    "print (\"cleaned1:\\n\", cleaned1[:50])\n",
    "print(\"---\")\n",
    "\n",
    "##  Cleanup 3 - stem\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "## TODO : iterate through 'cleaned1'\n",
    "cleaned2 = [stemmer.stem(word) for word in cleaned1]\n",
    "print (\"len(cleaned2) : \", len(cleaned2))\n",
    "print (\"cleaned2:\\n\", cleaned2[:50])\n",
    "print(\"---\")\n",
    "\n",
    "## TODO : Wordcount on 'cleaned2'\n",
    "wc = FreqDist(cleaned2)\n",
    "print(\"top 20 word count(cleaned2) : \" )\n",
    "for word, frequency in wc.most_common(20):\n",
    "    print(f\"{word} = {frequency}\")\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
