{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: TXT-2: N-grams\n",
    "\n",
    "## Overview\n",
    "Process N-grams\n",
    "\n",
    "## Run time \n",
    "20 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 460 ms, sys: 240 ms, total: 700 ms\n",
      "Wall time: 556 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import nltk\n",
    "# nltk.download('all')\n",
    "\n",
    "## downloading using command line\n",
    "#! python -m nltk.downloader all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw text ( 212 ) : \n",
      " It was a sunny day! We went to the dog park.  Lots of dogs were running around.  \n",
      "My dog likes to run too; so he had a great time.  \n",
      "I bought ice cream from the ice cream truck. Yummy!\n",
      "It was a perfect sunny day!\n",
      "---\n",
      "words ( 53 ) : \n",
      " ['It', 'was', 'a', 'sunny', 'day', '!', 'We', 'went', 'to', 'the', 'dog', 'park', '.', 'Lots', 'of', 'dogs', 'were', 'running', 'around', '.', 'My', 'dog', 'likes', 'to', 'run', 'too', ';', 'so', 'he', 'had', 'a', 'great', 'time', '.', 'I', 'bought', 'ice', 'cream', 'from', 'the', 'ice', 'cream', 'truck', '.', 'Yummy', '!', 'It', 'was', 'a', 'perfect', 'sunny', 'day', '!']\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# from os.path import expanduser\n",
    "# nltk.data.path.append( expanduser(\"~\") + \"/data/nltk_data\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pprint import pprint\n",
    "\n",
    "text = \"\"\"It was a sunny day! We went to the dog park.  Lots of dogs were running around.  \n",
    "My dog likes to run too; so he had a great time.  \n",
    "I bought ice cream from the ice cream truck. Yummy!\n",
    "It was a perfect sunny day!\"\"\"\n",
    "\n",
    "words = word_tokenize(text)\n",
    "words_lower = [i.lower() for i in words]\n",
    "\n",
    "print (\"raw text (\", len(text) , \") : \\n\", text)\n",
    "print (\"---\")\n",
    "print (\"words (\" , len(words), \") : \\n\",words)\n",
    "print (\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in raw text\n",
      "[(('It', 'was'), 2),\n",
      " (('was', 'a'), 2),\n",
      " (('sunny', 'day'), 2),\n",
      " (('day', '!'), 2),\n",
      " (('ice', 'cream'), 2),\n",
      " (('a', 'sunny'), 1),\n",
      " (('!', 'We'), 1),\n",
      " (('We', 'went'), 1),\n",
      " (('went', 'to'), 1),\n",
      " (('to', 'the'), 1),\n",
      " (('the', 'dog'), 1),\n",
      " (('dog', 'park'), 1),\n",
      " (('park', '.'), 1),\n",
      " (('.', 'Lots'), 1),\n",
      " (('Lots', 'of'), 1),\n",
      " (('of', 'dogs'), 1),\n",
      " (('dogs', 'were'), 1),\n",
      " (('were', 'running'), 1),\n",
      " (('running', 'around'), 1),\n",
      " (('around', '.'), 1),\n",
      " (('.', 'My'), 1),\n",
      " (('My', 'dog'), 1),\n",
      " (('dog', 'likes'), 1),\n",
      " (('likes', 'to'), 1),\n",
      " (('to', 'run'), 1),\n",
      " (('run', 'too'), 1),\n",
      " (('too', ';'), 1),\n",
      " ((';', 'so'), 1),\n",
      " (('so', 'he'), 1),\n",
      " (('he', 'had'), 1),\n",
      " (('had', 'a'), 1),\n",
      " (('a', 'great'), 1),\n",
      " (('great', 'time'), 1),\n",
      " (('time', '.'), 1),\n",
      " (('.', 'I'), 1),\n",
      " (('I', 'bought'), 1),\n",
      " (('bought', 'ice'), 1),\n",
      " (('cream', 'from'), 1),\n",
      " (('from', 'the'), 1),\n",
      " (('the', 'ice'), 1),\n",
      " (('cream', 'truck'), 1),\n",
      " (('truck', '.'), 1),\n",
      " (('.', 'Yummy'), 1),\n",
      " (('Yummy', '!'), 1),\n",
      " (('!', 'It'), 1),\n",
      " (('a', 'perfect'), 1),\n",
      " (('perfect', 'sunny'), 1)]\n"
     ]
    }
   ],
   "source": [
    "bigrams = nltk.ngrams(words, 2)\n",
    "fdist = nltk.FreqDist(bigrams)\n",
    "print(\"bigrams in raw text\")\n",
    "pprint(fdist.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned words ( 24 ):\n",
      " ['sunny', 'day', 'went', 'dog', 'park', 'lots', 'dogs', 'running', 'around', 'dog', 'likes', 'run', 'great', 'time', 'bought', 'ice', 'cream', 'ice', 'cream', 'truck', 'yummy', 'perfect', 'sunny', 'day']\n",
      "---\n",
      "bigrams in cleaned text\n",
      "[(('sunny', 'day'), 2),\n",
      " (('ice', 'cream'), 2),\n",
      " (('day', 'went'), 1),\n",
      " (('went', 'dog'), 1),\n",
      " (('dog', 'park'), 1),\n",
      " (('park', 'lots'), 1),\n",
      " (('lots', 'dogs'), 1),\n",
      " (('dogs', 'running'), 1),\n",
      " (('running', 'around'), 1),\n",
      " (('around', 'dog'), 1),\n",
      " (('dog', 'likes'), 1),\n",
      " (('likes', 'run'), 1),\n",
      " (('run', 'great'), 1),\n",
      " (('great', 'time'), 1),\n",
      " (('time', 'bought'), 1),\n",
      " (('bought', 'ice'), 1),\n",
      " (('cream', 'ice'), 1),\n",
      " (('cream', 'truck'), 1),\n",
      " (('truck', 'yummy'), 1),\n",
      " (('yummy', 'perfect'), 1),\n",
      " (('perfect', 'sunny'), 1)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## TODO : Now cleanup STOP words and calculate bigrams again\n",
    "\n",
    "stop_words_english = set(stopwords.words('english'))\n",
    "stop_words_english.update(['-', '.', ',', '#', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}']) \n",
    "\n",
    "cleaned = [i for i in words_lower if i not in stop_words_english]\n",
    "print (\"cleaned words (\", len(cleaned), \"):\\n\", cleaned)\n",
    "print (\"---\")\n",
    "\n",
    "## Complete the following\n",
    "bigrams = nltk.ngrams(cleaned, 2)\n",
    "fdist = nltk.FreqDist(bigrams)\n",
    "print(\"bigrams in cleaned text\")\n",
    "pprint(fdist.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Find top bigrams of following text\n",
    "\n",
    "State of The Union - location : ../data/text/sotu-2014-obama.txt\n",
    "Moby Dick : ../data/text/moby-dick.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the text as follows\n",
    "f = open('/data/text/state-of-the-unions/2014-Obama.txt')\n",
    "text = f.read()\n",
    "\n",
    "# TODO - continue from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - NGram Applications\n",
    "Ngrams can be used for text prediction.\n",
    "\n",
    "For example, if I am texting some one  \n",
    "__\"pleae call me ???\"__    \n",
    "What is the final word?\n",
    "\n",
    "Using ngram analytics of lots of text messages we can deduce that final word can be\n",
    "- 'back' ,  with 90% probability\n",
    "- or 'asap' , with 60% probability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
